"""
D√âTECTEUR DE RUMEURS - VERSION AM√âLIOR√âE
Am√©liorations:
1. Topics configurables (JSON ou param√®tres API)
2. Score de viralit√© corrig√© (5 apparitions = viral)
3. Filtrage strict sur le B√©nin uniquement
4. Les rumeurs non-virales ne sont PAS enregistr√©es
"""

import os
import json
import logging
import time
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import requests
from dotenv import load_dotenv
from typing import Optional
import re

load_dotenv()

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except Exception:
    GEMINI_AVAILABLE = False

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# ==========================================
# CONFIGURATION
# ==========================================
class Config:
    GOOGLE_SEARCH_API_KEY = os.getenv("GOOGLE_SEARCH_API_KEY", "")
    GOOGLE_SEARCH_ENGINE_ID = os.getenv("GOOGLE_SEARCH_ENGINE_ID", "")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

    # Sites FIABLES (pour V√âRIFIER les rumeurs)
    TRUSTED_SOURCES = [
        # --- M√©dias b√©ninois principaux ---
        "beninwebtv.com",
        "lematinal.bj",
        "ortb.bj",
        "lanouvelletribune.info",
        "lanation.bj",
        "24haubenin.info",
        "actubenin.com",
        "matinlibre.com",
        "lepoint.bj",
        "la-quotidienne.bj",
        "banouto.bj",
        "fraternitefj.com",
        "fraternitefm.bj",
        "jupiterinfo.bj",
        "firstafriquetv.bj",
        "leleaderinfobenin.bj",
        "gueritetvmonde.bj",
        "bjnews.bj",
        "diffo.net",
        "lepotentiel.bj",
        "eketinmagazine.com",

        # --- M√©dias panafricains fiables ---
        "rfi.fr",
        "bbc.com",
        "dw.com",
        "jeuneafrique.com",
        "africanews.com",
        "rtb.bf",   # Burkina Faso (souvent repris au B√©nin)
        "lefaso.net",
        "linfodrome.com",   # C√¥te d‚ÄôIvoire s√©rieux
        "seneweb.com",      # S√©n√©gal
        "lequotidien.sn",

        # --- Fact-checkers (tr√®s importants pour ton syst√®me !) ---
        "africacheck.org",
        "benincheck.info",      # (site existant dans certains projets)
        "dubawa.org",           # Afrique de l‚ÄôOuest
        "factcheck.org",
        "fullfact.org",
        "snopes.com",

        # --- Institutions b√©ninoises ---
        "gouv.bj",
        "presidence.bj",
        "assemblee-nationale.bj",
        "justice.gouv.bj",
        "finances.bj",
        "msp.bj",               # Minist√®re de la Sant√©
        "interieur.gouv.bj",
        "police.bj",

        # --- Organisations internationales ---
        "who.int",
        "un.org",
        "worldbank.org",
        "imf.org",
        "unodc.org",
        "ecowas.int",
    ]


    # Sites √† IGNORER compl√®tement
    BLACKLIST = ["archive.org", "webcache.googleusercontent.com"]
    
    OUTPUT_FILE = "rumors_detected.json"
    TOPICS_FILE = "topics.json"  # Fichier de configuration des topics
    RECENT_DAYS = 360
    # Limites pour prot√©ger l'API de recherche (par d√©faut raisonnable)
    MAX_SEARCH_REQUESTS = 10
    QUERY_DELAY = 2.0 # secondes entre requ√™tes
    
    # NOUVEAU: Score de viralit√© corrig√©
    VIRALITY_THRESHOLD = 0.50  # Seuil minimal pour v√©rifier (50%)
    MIN_OCCURRENCES_FOR_VIRAL = 5  # 5 apparitions = r√©ellement viral
    
    # NOUVEAU: Mots-cl√©s B√©nin (filtrage intelligent)
    BENIN_KEYWORDS = [
        # Pays
        "b√©nin", "benin", "b√©ninois", "beninois", "b√©ninoises",
        # Villes principales
        "cotonou", "cotonnou", "porto-novo", "porto novo", "parakou",
        "abomey", "bohicon", "natitingou", "djougou", "lokossa", "ouidah",
        # R√©gions
        "ou√©m√©", "atlantique", "borgou", "zou", "mono", "couffo", 
        "collines", "plateau", "atacora", "donga", "alibori", "littoral",
        # Personnalit√©s et institutions
        "patrice talon", "talon", "gouvernement b√©ninois", "ceni b√©nin",
        "assembl√©e nationale b√©nin", "pr√©sidence b√©nin"
    ]
    
    # Topics par d√©faut si aucun fichier n'est fourni
    DEFAULT_TOPICS = {
        "politique": ["Patrice Talon", "CENI", "√©lections", "gouvernement b√©ninois"],
        "sant√©": ["vaccination B√©nin", "chol√©ra B√©nin", "paludisme B√©nin"],
        "√©conomie": ["carburant B√©nin", "prix B√©nin", "CFA"],
        "s√©curit√©": ["s√©curit√© B√©nin", "braquage Cotonou"]
    }


def domain_of_url(url: str) -> str:
    """Extrait le domaine d'une URL"""
    if not url:
        return ""
    try:
        from urllib.parse import urlparse
        host = urlparse(url).hostname or ""
        host = host.lower()
        if host.startswith("www."):
            host = host[4:]
        return host
    except Exception:
        return url.lower()


def is_about_benin(text: str) -> bool:
    """
    NOUVEAU: V√©rifie si le texte concerne vraiment le B√©nin
    Approche flexible : cherche mentions du B√©nin
    """
    if not text:
        return False
    
    text_lower = text.lower()
    
    # Enlever accents pour une meilleure d√©tection
    import unicodedata
    text_normalized = unicodedata.normalize('NFD', text_lower)
    text_normalized = ''.join(c for c in text_normalized if unicodedata.category(c) != 'Mn')
    
    # Chercher dans le texte original ET normalis√©
    for keyword in Config.BENIN_KEYWORDS:
        keyword_normalized = unicodedata.normalize('NFD', keyword.lower())
        keyword_normalized = ''.join(c for c in keyword_normalized if unicodedata.category(c) != 'Mn')
        
        if keyword in text_lower or keyword_normalized in text_normalized:
            return True
    
    return False


# ==========================================
# GEMINI EMBEDDINGS
# ==========================================
class GeminiEmbedder:
    def __init__(self, api_key: str):
        self.api_key = api_key
        if GEMINI_AVAILABLE and api_key:
            genai.configure(api_key=api_key)
    
    def embed_text(self, text: str) -> List[float]:
        if not GEMINI_AVAILABLE or not self.api_key:
            return [float(hash(text) % 1000) / 1000.0]
        
        try:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document"
            )
            return result['embedding']
        except Exception as e:
            logging.error(f"Embedding error: {e}")
            return [float(hash(text) % 1000) / 1000.0]
    
    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        try:
            dot_prod = sum(a * b for a, b in zip(vec1, vec2))
            norm1 = sum(a * a for a in vec1) ** 0.5
            norm2 = sum(b * b for b in vec2) ** 0.5
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return float(dot_prod) / (norm1 * norm2)
        except Exception:
            return 0.0


# ==========================================
# D√âDUPLICATEUR
# ==========================================
class RumorDeduplicator:
    def __init__(self, similarity_threshold: float = 0.85):
        self.embedder = GeminiEmbedder(Config.GEMINI_API_KEY)
        self.rumor_embeddings = []
        self.similarity_threshold = similarity_threshold
    
    def is_duplicate(self, rumor_text: str) -> bool:
        if not rumor_text.strip():
            return True
        
        new_emb = self.embedder.embed_text(rumor_text)
        
        for entry in self.rumor_embeddings:
            sim = self.embedder.cosine_similarity(new_emb, entry["embedding"])
            if sim >= self.similarity_threshold:
                logging.info(f"   Duplicate (sim: {sim:.2f})")
                return True
        
        self.rumor_embeddings.append({
            "text": rumor_text,
            "embedding": new_emb,
            "date_added": datetime.now().isoformat()
        })
        return False


# ==========================================
# EXTRACTEUR DE RUMEURS
# ==========================================
class RumorExtractor:
    """D√©tecte les rumeurs dans les r√©sultats de recherche"""
    
    RUMOR_INDICATORS = [
        "rumeur", "info ou intox", "est-ce vrai", "circule",
        "on dit que", "selon des sources", "non confirm√©",
        "aurait", "para√Æt que", "fake news"
    ]
    
    DEMENTI_KEYWORDS = [
        "d√©ment", "d√©menti", "r√©fute", "fausse rumeur",
        "clarification", "mise au point", "infirme"
    ]
    
    @classmethod
    def is_rumor_candidate(cls, text: str) -> bool:
        """V√©rifie si c'est une rumeur potentielle"""
        text_lower = text.lower()
        
        if any(kw in text_lower for kw in cls.DEMENTI_KEYWORDS):
            return False
        
        return any(ind in text_lower for ind in cls.RUMOR_INDICATORS)
    

    @classmethod
    def extract_rumor_text(cls, title: str, snippet: str) -> Optional[str]:
        """Extrait le texte complet de la rumeur sans tronquer"""
        # Concat√®ne titre + snippet
        full_text = f"{title} {snippet}".strip()

        # V√©rifie que c'est une rumeur potentielle
        if not cls.is_rumor_candidate(full_text):
            return None

        # Nettoyage minimal : enlever dates et URLs, mais pas tronquer
        clean = re.sub(r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}', '', full_text)
        clean = re.sub(r'http\S+', '', clean)
        clean = re.sub(r'\s+', ' ', clean).strip()

        # Retourne tout le texte disponible
        return clean if len(clean) > 20 else None


# ==========================================
# VIRALITY SCORER (CORRIG√â)
# ==========================================
class ViralityScorer:
    """
    NOUVEAU: Score de viralit√© corrig√©
    5 apparitions = 1.0 (100% viral)
    """
    def score(self, rumor_text: str, occurrences: int) -> float:
        """
        Calcule un score de viralit√© entre 0 et 1
        - occurrences: nombre de sites o√π la rumeur appara√Æt
        - 5 occurrences = 100% viral
        """
        # Score de base: 5 sources = 100%
        base = min(occurrences / Config.MIN_OCCURRENCES_FOR_VIRAL, 1.0)
        
        # Bonus selon mots viraux
        viral_words = ["circule", "buzz", "choc", "explose", "panique", "alerte"]
        bonus = 0.1 * sum(1 for w in viral_words if w in rumor_text.lower())
        
        return min(base + bonus, 1.0)


def jaccard_similarity(a: str, b: str) -> float:
    """Compute Jaccard similarity on word tokens"""
    import re
    a_set = set(re.findall(r"\w+", (a or "").lower()))
    b_set = set(re.findall(r"\w+", (b or "").lower()))
    if not a_set or not b_set:
        return 0.0
    return len(a_set & b_set) / len(a_set | b_set)


# ==========================================
# QUERY GENERATOR (AVEC TOPICS CONFIGURABLES)
# ==========================================
class QueryGenerator:
    """
    NOUVEAU: G√©n√®re des requ√™tes bas√©es sur des topics configurables
    """
    def __init__(self, topics: Optional[Dict[str, List[str]]] = None):
        """
        Args:
            topics: Dictionnaire {categorie: [liste de topics]}
                   Si None, utilise les topics par d√©faut
        """
        if topics is None:
            # Essayer de charger depuis le fichier
            topics = self.load_topics_from_file()
        
        self.topics = topics or Config.DEFAULT_TOPICS
        
        # Indicateurs de rumeurs (pour certaines requ√™tes)
        self.rumor_indicators = ["rumeur", "info ou intox", "circule", "fake news"]
        
        # Mots-cl√©s g√©n√©raux pour trouver aussi de l'actualit√© r√©cente
        self.general_indicators = ["actualit√©", "news", "derni√®re minute", "breaking"]
        
        # Compatibilit√©: regrouper tous les indicateurs dans `self.indicators`
        # (la m√©thode generate_queries utilisait `self.indicators`)
        self.indicators = self.rumor_indicators + self.general_indicators
        
        logging.info(f"üìã Topics charg√©s: {list(self.topics.keys())}")
    
    @staticmethod
    def load_topics_from_file(filename: str = Config.TOPICS_FILE) -> Optional[Dict]:
        """Charge les topics depuis un fichier JSON"""
        if not os.path.exists(filename):
            logging.info(f"‚ö†Ô∏è  Fichier {filename} non trouv√©, utilisation des topics par d√©faut")
            return None
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                topics = json.load(f)
            logging.info(f"‚úÖ Topics charg√©s depuis {filename}")
            return topics
        except Exception as e:
            logging.error(f"‚ùå Erreur lecture {filename}: {e}")
            return None
    
    def generate_queries(self, max_queries: int = 10) -> List[str]:
        """G√©n√®re des requ√™tes de recherche"""
        queries = []
        
        # NOUVEAU: G√©n√©rer VRAIMENT jusqu'√† max_queries
        for category, topics in self.topics.items():
            for topic in topics:  # TOUS les topics, pas juste [:4]
                for indicator in self.indicators:  # TOUS les indicateurs
                    # Ajouter "B√©nin" dans la requ√™te
                    query = f'"{topic}" "{indicator}" B√©nin -site:gouv.bj'
                    queries.append(query)
                    
                    if len(queries) >= max_queries:
                        return queries[:max_queries]
        
        return queries
    
    @staticmethod
    def save_default_topics(filename: str = Config.TOPICS_FILE):
        """Sauvegarde les topics par d√©faut dans un fichier (utilitaire)"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(Config.DEFAULT_TOPICS, f, ensure_ascii=False, indent=2)
        logging.info(f"‚úÖ Topics par d√©faut sauvegard√©s dans {filename}")


# ==========================================
# WEB SEARCHER
# ==========================================
class WebSearcher:
    def __init__(self, api_key: str, engine_id: str, max_requests: Optional[int] = None, query_delay: float = 0.5):
        self.api_key = api_key
        self.engine_id = engine_id
        self._max_requests = max_requests
        self._requests_made = 0
        self._query_delay = query_delay
    
    def search(self, query: str, num_results: int = 10) -> List[Dict]:
        """Recherche sur le web"""
        if not self.api_key:
            return []
        params = {
            "key": self.api_key,
            "cx": self.engine_id,
            "q": query,
            "num": min(num_results, 10)
        }

        # Retry with exponential backoff on 429 or transient network errors
        max_retries = 3
        backoff = 1
        url = "https://www.googleapis.com/customsearch/v1"

        for attempt in range(1, max_retries + 1):
            # Respect global max requests budget
            if self._max_requests is not None and self._requests_made >= self._max_requests:
                logging.warning(f"Search budget exhausted ({self._requests_made}/{self._max_requests}); skipping query.")
                return []

            # Optional delay between queries to avoid burst
            if self._query_delay and attempt == 1:
                time.sleep(self._query_delay)
            try:
                resp = requests.get(url, params=params, timeout=12)
                # Count this request attempt
                self._requests_made += 1

                # Handle explicit 429 with Retry-After if provided
                if resp.status_code == 429:
                    retry_after = resp.headers.get("Retry-After")
                    try:
                        wait = int(retry_after) if retry_after is not None else backoff
                    except Exception:
                        wait = backoff
                    logging.warning(f"Search 429 Too Many Requests; retrying after {wait}s (attempt {attempt}/{max_retries})")
                    time.sleep(wait)
                    backoff *= 2
                    continue

                resp.raise_for_status()
                data = resp.json()

                return [
                    {
                        "title": item.get("title", ""),
                        "snippet": item.get("snippet", ""),
                        "link": item.get("link", ""),
                        "displayLink": item.get("displayLink", "")
                    }
                    for item in data.get("items", [])
                ]

            except requests.exceptions.RequestException as e:
                logging.error(f"Search error (attempt {attempt}/{max_retries}): {e}")
                if attempt < max_retries:
                    logging.info(f"Retrying in {backoff}s...")
                    time.sleep(backoff)
                    backoff *= 2
                    continue
                else:
                    logging.error("Max retries reached for search; giving up on this query.")
                    return []

        return []


# ==========================================
# FACT CHECKER
# ==========================================
from bs4 import BeautifulSoup
class ImprovedFactChecker:
    """Fact-checker avec fetch complet des pages et analyse Gemini intelligente"""
    
    def __init__(self, api_key: Optional[str], searcher):
        self.api_key = os.getenv("GEMINI_API_KEY", "")
        self.searcher = searcher
        
        if not api_key:
            logging.error("‚ùå GEMINI_API_KEY manquante - V√©rification impossible!")
            self.gemini_available = False
        elif not GEMINI_AVAILABLE:
            logging.error("‚ùå Module google.generativeai non install√©!")
            self.gemini_available = False
        else:
            try:
                genai.configure(api_key=api_key)
                self.gemini_available = True
                logging.info("‚úÖ Gemini configur√© et pr√™t")
            except Exception as e:
                logging.error(f"‚ùå Erreur configuration Gemini: {e}")
                self.gemini_available = False
    
    # ==========================================
    # √âTAPE 1: RECHERCHE DE SOURCES FIABLES
    # ==========================================
    
    def find_trusted_sources(self, rumor_text: str) -> List[Dict]:
        """Recherche multi-passes pour trouver des sources fiables"""
        logging.info(f"   üîç Recherche sources fiables: {rumor_text[:80]}...")
        
        # Extraire mots-cl√©s pertinents
        keywords = self._extract_keywords(rumor_text)
        all_sources = []
        
        # PASSE 1: M√©dias b√©ninois + mots-cl√©s
        benin_media = ["beninwebtv.com", "lematinal.bj", "lanation.bj", "24haubenin.info", "ortb.bj"]
        query1 = f'{keywords} ({" OR ".join([f"site:{d}" for d in benin_media])})'
        results1 = self.searcher.search(query1, num_results=8)
        all_sources.extend(results1)
        
        # PASSE 2: Sites officiels
        if len(all_sources) < 5:
            official = ["gouv.bj", "presidence.bj"]
            query2 = f'{keywords} ({" OR ".join([f"site:{d}" for d in official])})'
            results2 = self.searcher.search(query2, num_results=5)
            all_sources.extend(results2)
        
        # PASSE 3: Recherche large avec contexte
        if len(all_sources) < 3:
            query3 = f'{keywords} B√©nin (officiel OR confirm√© OR d√©menti OR annonce)'
            results3 = self.searcher.search(query3, num_results=10)
            all_sources.extend(results3)
        
        logging.info(f"   üìä {len(all_sources)} sources trouv√©es")
        return all_sources[:10]  # Max 10 sources
    
    # ==========================================
    # EXTRACTION DE MOTS-CL√âS
    # ==========================================

    def _extract_keywords(self, text: str) -> str:
        """
        Extraction intelligente des mots-cl√©s.
        Utilise Gemini si disponible, sinon fallback simple.
        """
        # Si Gemini n‚Äôest pas dispo
        if not self.gemini_available:
            # Fallback simple : garder mots > 4 lettres
            tokens = re.findall(r"\b\w+\b", text.lower())
            filtered = [t for t in tokens if len(t) > 4]
            return " ".join(filtered[:6])  # max 6 mots

        try:
            model = genai.GenerativeModel("gemini-1.5-flash")
            prompt = f"""
            Extrait les mots-cl√©s principaux de ce texte.
            Retourne UNIQUEMENT une liste de 3 √† 7 mots-cl√©s s√©par√©s par des espaces.

            Texte :
            {text}
            """
            response = model.generate_content(prompt)
            keywords = response.text.strip()


            # Nettoyage
            keywords = re.sub(r"[^a-zA-Z0-9√Ä-√ø \-]", "", keywords)
            return keywords

        except:
            # Si Gemini crashe, fallback simple
            tokens = re.findall(r"\b\w+\b", text.lower())
            filtered = [t for t in tokens if len(t) > 4]
            return " ".join(filtered[:6])


    # ==========================================
    # √âTAPE 2: FETCH COMPLET DES PAGES
    # ==========================================
    
    def fetch_full_content(self, url: str) -> Optional[str]:
        """
        R√©cup√®re le contenu COMPLET d'une page web
        (pas juste le snippet Google)
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; FactCheckBot/1.0)',
                'Accept': 'text/html,application/xhtml+xml',
                'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8'
            }
            
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()
            
            # Parser avec BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraire le texte de l'article (supprimer scripts, styles, etc.)
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # R√©cup√©rer le texte principal
            text = soup.get_text(separator=' ', strip=True)
            
            # Nettoyer
            text = re.sub(r'\s+', ' ', text)
            text = text[:8000]  # Limiter √† 8000 caract√®res (pour Gemini)
            
            logging.info(f"      ‚úÖ Fetch√©: {len(text)} caract√®res de {url[:50]}...")
            return text
        
        except Exception as e:
            logging.warning(f"      ‚ö†Ô∏è Erreur fetch {url[:50]}: {e}")
            return None
    
    def fetch_all_sources(self, sources: List[Dict]) -> List[Dict]:
        """Fetch le contenu complet de toutes les sources"""
        enriched_sources = []
        
        for src in sources[:5]:  # Limiter √† 5 pour √©viter trop de requ√™tes
            url = src.get("link", "")
            if not url:
                continue
            
            full_content = self.fetch_full_content(url)
            
            enriched_sources.append({
                "url": url,
                "domain": src.get("displayLink", ""),
                "title": src.get("title", ""),
                "snippet": src.get("snippet", ""),
                "full_content": full_content or src.get("snippet", "")  # Fallback sur snippet
            })
        
        return enriched_sources
    

    def _fallback_verification(self, rumor_text: str, trusted_sources: List[Dict]) -> Dict:
        """
        Fallback simple si Gemini n'est pas dispo ou erreur
        Bas√© sur la pr√©sence de mots-cl√©s dans les snippets
        """
        positive_indicators = ["confirm√©", "vrai", "officiel", "annonc√©"]
        negative_indicators = ["d√©menti", "faux", "infond√©", "r√©fute"]
        
        score = 0
        for src in trusted_sources:
            snippet = src.get("snippet", "").lower()
            if any(word in snippet for word in positive_indicators):
                score += 1
            if any(word in snippet for word in negative_indicators):
                score -= 1
        
        if score > 0:
            verdict = "vrai"
            confidence = min(0.5 + 0.1 * score, 0.9)
        elif score < 0:
            verdict = "faux"
            confidence = min(0.5 + 0.1 * abs(score), 0.9)
        else:
            verdict = "non v√©rifiable"
            confidence = 0.5
        
        logging.info(f"   ‚úÖ Fallback verdict: {verdict} (score: {confidence:.2f})")
        
        return {
            "verdict": verdict,
            "confidence": confidence,
            "reasoning": "Analyse bas√©e sur les snippets des sources fiables.",
            "sources_used": trusted_sources,
        }
    


    def extract_json(self, text: str) -> dict | None:
        """
        Essaie d'extraire le JSON depuis un texte brut renvoy√© par Gemini.
        Retourne None si aucun JSON valide n'est trouv√©.
        """
        # Nettoyer guillemets typographiques si jamais
        text = text.replace("‚Äú", '"').replace("‚Äù", '"')
        
        # Extraire le premier bloc JSON { ... }
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if not match:
            return None
        
        json_text = match.group(0)
        
        try:
            return json.loads(json_text)
        except json.JSONDecodeError:
            return None

    
    # ==========================================
    # √âTAPE 3: V√âRIFICATION AVEC GEMINI
    # ==========================================
    
    def verify_with_gemini(self, rumor_text: str, trusted_sources: List[Dict]) -> Dict:
        """
        V√©rification intelligente avec Gemini
        Analyse le CONTENU COMPLET des sources et utilise le r√©sum√© JSON de Gemini pour d√©cider.
        """
        if not self.gemini_available:
            logging.warning("‚ö†Ô∏è Gemini non disponible, utilisation fallback")
            return self._fallback_verification(rumor_text, trusted_sources)
        
        # √âtape 1: Fetch le contenu complet
        logging.info("üì• Fetch du contenu complet des sources...")
        enriched_sources = self.fetch_all_sources(trusted_sources)
        
        if not enriched_sources:
            logging.warning("‚ö†Ô∏è Aucun contenu r√©cup√©r√©")
            return self._fallback_verification(rumor_text, trusted_sources)
        
        # √âtape 2: Pr√©parer le contexte pour Gemini
        context = self._build_context(enriched_sources)
        
        # √âtape 3: Construire le prompt intelligent
        prompt = self._build_intelligent_prompt(rumor_text, context)
        
        try:
            # Appel de Gemini 2.0
            model = genai.GenerativeModel("gemini-2.0-flash")
            logging.info("ü§ñ Appel Gemini pour analyse...")
            response = model.generate_content(prompt)
            text = response.text.strip()
            print(text)  # Pour debug
            # √âtape 4: Parser le JSON renvoy√© par Gemini
            try:
                gemini_result = self.extract_json(text)
            except json.JSONDecodeError:
                logging.warning("‚ö†Ô∏è R√©ponse Gemini non JSON, fallback utilis√©")
                return self._fallback_verification(rumor_text, enriched_sources)
            

            score = gemini_result.get("score_veracite", 0.5)
            if score <= 0.49:
                verdict = "FAUX"
            elif score == 0.5:
                verdict = "INCERTAIN"
            else:  # 0.51 <= score <= 1
                verdict = "VRAI"
            
            # √âtape 5: Construire le r√©sultat final
            result = {
                "verdict": verdict,
                "score_veracite": gemini_result.get("score_veracite", 0.5),
                "explication": gemini_result.get("explication", ""),
                "sources_utilisees": gemini_result.get("sources_utilisees", []),
                "elements_cles": gemini_result.get("elements_cles", []),
                "recommandation": gemini_result.get("recommandation", "")
            }
            
            logging.info(f"‚úÖ Gemini verdict: {result['verdict']} (score: {result['score_veracite']:.2f})")
            return result
        
        except Exception as e:
            logging.error(f"‚ùå Erreur Gemini: {e}")
            return self._fallback_verification(rumor_text, trusted_sources)

    
    # ==========================================
    # PROMPT INTELLIGENT POUR GEMINI
    # ==========================================
    
    def _build_intelligent_prompt(self, rumor_text: str, context: str) -> str:
        """
        Construit un prompt pour Gemini 2.0 Flash qui :
        - R√©sume le contenu complet des sources
        - V√©rifie la v√©racit√© de la rumeur
        - Fournit un verdict clair et justifi√©
        """
        current_year = datetime.now().year

        return f"""Tu es un fact-checker expert sp√©cialis√© dans les rumeurs au B√©nin. 

            RUMEUR √Ä V√âRIFIER:
            "{rumor_text}"

            CONTEXTE (sources fiables analys√©es):
            {context}

            OBJECTIFS:
            1. R√©sumer le contenu principal des sources pour chaque point cl√©.
            2. V√©rifier la v√©racit√© de la rumeur selon les informations disponibles.
            3. Tenir compte du contexte temporel et l√©gal (dates, √©v√©nements pass√©s, Constitution, annonces officielles).
            4. Identifier tout √©l√©ment contradictoire ou incertain.

            FORMAT STRICT:
            Renvoie uniquement un JSON avec les champs suivants :

            {{
            "verdict": "VRAI/FAUX/INCERTAIN",
            "score_veracite": 0.0-1.0,
            "resume_sources": ["R√©sum√© clair de chaque source analys√©e"],
            "explication": "Analyse d√©taill√©e justifiant le verdict",
            "sources_utilisees": ["liste des URLs pertinentes"],
            "elements_cles": ["points cl√©s extraits des sources"],
            "recommandation": "Conseil/action √† prendre"
            }}

            EXEMPLE :
            {{
            "verdict": "FAUX",
            "score_veracite": 0.2,
            "resume_sources": ["Article 1: info 2021...", "Article 2: annonce d√©mentie..."],
            "explication": "Aucune source ne confirme la rumeur pour 2026. Sources disponibles concernent 2021.",
            "sources_utilisees": ["URL1", "URL2"],
            "elements_cles": ["Articles 2021", "Pas d'annonce 2026"],
            "recommandation": "Rumeur infond√©e"
            }}
        """

    
    def _build_context(self, enriched_sources: List[Dict]) -> str:
        """Construit le contexte avec le CONTENU COMPLET des sources"""
        context_parts = []
        
        for i, src in enumerate(enriched_sources, 1):
            context_parts.append(f"""
                SOURCE {i}: {src['domain']}
                URL: {src['url']}
                TITRE: {src['title']}
                CONTENU:
                {src['full_content'][:2000]}...
                ---
            """)
        
        return

# ==========================================
# SYST√àME PRINCIPAL
# ==========================================
class CorrectRumorDetectionSystem:
    def __init__(self, topics: Optional[Dict[str, List[str]]] = None, debug: bool = False, max_search_requests: Optional[int] = None, query_delay: float = 0.5):
        """
        Args:
            topics: Dictionnaire de topics personnalis√©s (optionnel)
            debug: Mode debug pour voir plus de d√©tails
        """
        self.query_generator = QueryGenerator(topics)
        self.web_searcher = WebSearcher(
            Config.GOOGLE_SEARCH_API_KEY,
            Config.GOOGLE_SEARCH_ENGINE_ID,
            max_requests=(max_search_requests or Config.MAX_SEARCH_REQUESTS),
            query_delay=(query_delay or Config.QUERY_DELAY)
        )
        self.extractor = RumorExtractor()
        self.deduplicator = RumorDeduplicator()
        self.virality_scorer = ViralityScorer()
        self.fact_checker = ImprovedFactChecker(Config.GEMINI_API_KEY, self.web_searcher)
        self.debug = debug
    
    def run_detection_cycle(self, max_queries: int = 10) -> List[Dict]:
        """Cycle complet de d√©tection

        Collecte TOUTES les rumeurs extraites (virales ou non). Les rumeurs
        non-virales auront un verdict `NON_VIRAL` et seront sauvegard√©es
        pour audit manuel.
        """
        queries = self.query_generator.generate_queries(max_queries)
        detected_rumors = []  # Toutes les rumeurs (virales ou NON)
        
        for i, query in enumerate(queries, 1):
            logging.info(f"\n{'='*70}")
            logging.info(f"üîç Recherche {i}/{len(queries)}: {query}")
            
            search_results = self.web_searcher.search(query, num_results=10)
            
            if not search_results:
                logging.info("   Aucun r√©sultat")
                continue
            
            # Filtrer sources non-v√©rifi√©es ET concernant le B√©nin
            unverified_sources = []
            for result in search_results:
                domain = domain_of_url(result.get("link", ""))
                
                if domain in Config.BLACKLIST:
                    continue
                
                if domain not in Config.TRUSTED_SOURCES:
                    # V√©rifier que le contenu concerne le B√©nin
                    full_text = f"{result.get('title', '')} {result.get('snippet', '')}"
                    
                    if self.debug:
                        logging.info(f"   üîç Analyse: {domain}")
                        logging.info(f"      Titre: {result.get('title', '')[:80]}")
                        logging.info(f"      Snippet: {result.get('snippet', '')[:80]}")
                    
                    if not is_about_benin(full_text):
                        logging.info(f"   ‚è≠Ô∏è  Ignor√© (pas sur le B√©nin): {domain}")
                        if self.debug:
                            logging.info(f"      Texte v√©rifi√©: {full_text[:100]}")
                        continue
                    
                    unverified_sources.append(result)
                    logging.info(f"   üìç Source non-v√©rifi√©e (B√©nin): {domain}")
            
            logging.info(f"   ‚û°Ô∏è  {len(unverified_sources)} sources non-v√©rifi√©es (B√©nin)")
            
            # Extraire les rumeurs
            for result in unverified_sources:
                rumor_text = self.extractor.extract_rumor_text(
                    result.get("title", ""),
                    result.get("snippet", "")
                )
                
                if not rumor_text:
                    continue
                
                if self.deduplicator.is_duplicate(rumor_text):
                    continue
                
                # Calculer viralit√©
                occurrences = 0
                for other in unverified_sources:
                    other_text = f"{other.get('title','')} {other.get('snippet','')}"
                    if jaccard_similarity(rumor_text, other_text) >= 0.25:
                        occurrences += 1

                virality_score = self.virality_scorer.score(rumor_text, occurrences)

                logging.info(f"   ‚ö†Ô∏è  RUMEUR: {rumor_text[:100]}...")
                logging.info(f"   üìå Source: {result.get('displayLink')}")
                logging.info(f"   üî• Viralit√©: {virality_score:.2f} ({occurrences} occurrences)")

                # Si la viralit√© est trop faible, on n'appelle pas le v√©rificateur
                # externe mais on ENREGISTRE quand m√™me la rumeur pour audit.
                if virality_score < Config.VIRALITY_THRESHOLD:
                    logging.info(f"   ‚è≠Ô∏è  Non-viral: viralit√© ({virality_score:.2f}) < seuil ({Config.VIRALITY_THRESHOLD})")

                    # On tente malgr√© tout une v√©rification heuristique l√©g√®re
                    # en recherchant sur les sources fiables et en utilisant
                    # la m√©thode de fallback pour produire un verdict.
                    try:
                        trusted_sources = self.fact_checker.find_trusted_sources(rumor_text)
                        verification = self.fact_checker._fallback_verification(rumor_text, trusted_sources)
                        # Indiquer que c'est une v√©rification heuristique (non-Gemini)
                        verification["verification_method"] = "heuristic_fallback"
                    except Exception as e:
                        logging.error(f"Erreur verification heuristique: {e}")
                        verification = {
                            "verdict": "INCERTAIN",
                            "score_veracite": 0.0,
                            "explication": "Erreur lors de la v√©rification heuristique",
                            "sources_utilisees": [],
                            "recommandation": "V√©rification manuelle",
                            "nb_sources_fiables": 0,
                            "verification_method": "heuristic_fallback"
                        }

                    record = {
                        "rumeur": rumor_text,
                        "virality_score": virality_score,
                        "occurrences": occurrences,
                        "source_non_verifiee": {
                            "domain": result.get("displayLink", ""),
                            "url": result.get("link", ""),
                            "titre": result.get("title", "")
                        },
                        "verification": verification,
                        "detected_at": datetime.now(timezone.utc).isoformat(),
                        "note": "NON_VIRAL"
                    }
                    detected_rumors.append(record)
                    continue

                # V√©rifier avec sources fiables
                logging.info(f"   üî¨ V√©rification...")
                trusted_sources = self.fact_checker.find_trusted_sources(rumor_text)
                verification = self.fact_checker.verify_with_gemini(rumor_text, trusted_sources)
                
                verdict = verification.get("verdict", "?")
                score = verification.get("score_veracite", 0)
                nb_sources = verification.get("nb_sources_fiables", 0)
                
                if verdict == "FAUX":
                    logging.info(f"   ‚ùå FAUX (score: {score:.2f}, {nb_sources} sources)")
                elif verdict == "VRAI":
                    logging.info(f"   ‚úÖ VRAI (score: {score:.2f}, {nb_sources} sources)")
                else:
                    logging.info(f"   ‚ö†Ô∏è  {verdict} ({nb_sources} sources)")
                
                # Enregistrer SEULEMENT si viral
                record = {
                    "rumeur": rumor_text,
                    "virality_score": virality_score,
                    "occurrences": occurrences,
                    "source_non_verifiee": {
                        "domain": result.get("displayLink", ""),
                        "url": result.get("link", ""),
                        "titre": result.get("title", "")
                    },
                    "verification": verification,
                    "detected_at": datetime.now(timezone.utc).isoformat()
                }
                
                detected_rumors.append(record)
                time.sleep(0.5)
        
        return detected_rumors
    
    def save_results(self, records: List[Dict], filename: str = Config.OUTPUT_FILE):
        """Sauvegarde les r√©sultats"""
        # √âcrire le fichier √† c√¥t√© du script pour √©viter les confusions de cwd
        base_dir = os.path.dirname(__file__) or os.getcwd()
        out_path = os.path.join(base_dir, filename)
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        logging.info(f"\nüíæ {len(records)} rumeurs sauvegard√©es: {out_path}")




